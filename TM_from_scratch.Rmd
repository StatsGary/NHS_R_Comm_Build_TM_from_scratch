---
title: "Building a TidyModel for Classification from scratch"
author: "Gary Hutson - Head of Machine Learning"

output:
  html_document:
    theme: lumen
    highlight: tango
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, echo=FALSE, include=TRUE}
library(MLDataR)
library(ConfusionTableR)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(skimr)
library(purrr)
library(ggthemes)
library(modeldata)
library(vetiver)

```

# Data preparation 

The following steps will show you the steps to prepare the data.

## Load in the dataaset and view statistics

The dataset for the example will be the Thyroid dataset contained in the `MLDataR` package.

```{r dataset_include}
td <- MLDataR::thyroid_disease
td <- td 
skim(td)
```

## Clean imports

We will remove the null values for now, but you could impute these with methods such as [MICE](https://scikit-learn.org/stable/modules/impute.html) or mean/mode/median imputation methods.

```{r clean_data}
td_clean <- td[complete.cases(td),]
dim(td_clean)
```

## View class distribution

Next we will view the class distribution of the classification task:

```{r class_disp}
table_class <- table(td_clean$ThryroidClass)
class_imbalance_original <- unclass(prop.table(table_class))[1:2]
print(class_imbalance_original)
```

We will do some over sampling on the sick cases later on in this tutorial, however this level of imbalance will lead to skewed ML models in terms of predicting most patients not to have a thyroid issue. 

Smote is the algorithm we will use for dealing with imbalance:

![](man/fig/test_smote.png)
This method is used to obtain a synthetically class-balanced or nearly class-balanced training set, which is then used to train the classifier. 

# Explaratory Data Analysis (EDA)

The EDA component we will build sources an external function from the functions sub folder in our project structure. This function builds the `histoplotter` function to enable the visualisation of our continuous variables. 

```{r distribution_function}
# Get continuous variables only
subset <- td_clean %>% 
  dplyr::select(ThryroidClass, patient_age, TSH_reading, T3_reading,
                T4_reading, thyrox_util_rate_T4U_reading,
                FTI_reading)


# Bring in external file for visualisations
source('functions/visualisations.R')

# Use plot function
plot <- histoplotter(subset, ThryroidClass, 
                     chart_x_axis_lbl = 'Thyroid Class', 
                     chart_y_axis_lbl = 'Measures',boxplot_color = 'navy', 
                     boxplot_fill = '#89CFF0', box_fill_transparency = 0.2) 

# Add extras to plot
plot + ggthemes::theme_solarized() + theme(legend.position = 'none') + 
  scale_color_manual(values=c('negative' = 'red', 'positive' = 'blue'))


```

As you can see - we have a number of outliers in the continuous variables. To deal with this we will apply a standardisation method to bring that variability on to a similar scale by mean centering, or another technique, to reduce the affects of the statistical outliers. Other treatment options could be to expunge these from the analysis via anomaly / outlier detection techniques. 

# Model preparation

The next set of steps will be used to get the data ready for the training of the models - we will have a baseline model and compare against a model known for tearing the tabular data challenges on Kaggle. 

## Dividing the data into train/val/test samples

Now we will divide the data into training, validation and test samples:

```{r divide_and_conquer}
td_clean <- td_clean %>% 
  dplyr::mutate(ThryroidClass = as.factor(ThryroidClass)) %>% 
  #dplyr::select(-ref_src) %>% 
  drop_na()

# Split the dataset 
td_split <-initial_split(td_clean, 
                                   strata = ThryroidClass, 
                                   prop=0.9,
                                   breaks = 4)

train <- training(td_split)
test <- testing(td_split)

```

Okay, we have the training and testing sample. This sample will be used to assess how accurate the model is on the held out testing set. This will link to the evaluate metrics for the model. We will delve into that later on in this training.

## Making the first recipe

<img src="man/fig/logo_recipes.png" width="200" height="300" align="left">

Recipes are a way to simplify the feature engineering process. Back in the old days you had to do each of these steps to the training data prior to fitting a model, especially using package such as `caret`. Now, you can speed this process up massively with the help of the recipes package. Let's whip up the first recipe:

```{r training_recipe}

train_rcp <- recipes::recipe(ThryroidClass ~ ., data=train) %>% 
  step_dummy(ref_src, role='predictor') %>% 
  # Upsample data
  themis::step_smote(ThryroidClass, over_ratio = 0.97, neighbors = 3) %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors())  
 # Remove predictors that have little effect on target


# Prep and bake the recipe so we can view this as a seperate data frame
training_df <- train_rcp %>% 
  prep() %>% 
  juice()

# Class imbalance resolved
class_imbalance_after_smote <- unclass(prop.table(table(training_df$ThryroidClass)))[1:2]


```

As we applied Synthetic Minority Oversampling - which is a nearest neighbours method of oversampling we need to check what has happened to the binary labels (negative or sick):

```{r class_imbalance}
imbalance_frame <- tibble(class_imbalance_original,
           class_imbalance_after_smote)

print(imbalance_frame)

```
This technique is not always successful, due to the severity of the imbalance, the representation of the sick class might make the overall distribution imbalanced. 

# Model training 

In this example I will create a baseline model and compare against one further classifier, for the sake of brevity. However, in ML challenges it is common to try many different classifiers and pit them against each other in the evaluation stages. 


## Training the Logistic Regression baseline model

The theory is that if a simple linear classifier does a better job than a more complex algorithm, then stick with good old logistic regression. I won't cover the mathematics of logistic regression, but it follows very closely to a linear regression equation, with the addition that there is a log link function used to turn it from a regressor into a classifier.

### Initialising the model

Here I use [Parsnip](https://parsnip.tidymodels.org/) to search for the list of [available models](https://www.tidymodels.org/find/parsnip/):

```{r logistic_reg}
lr_mod <- parsnip::logistic_reg() %>% 
  set_engine('glm')

print(lr_mod)

```

### Creating the model workflow

We will use workflows to create the model workflow:

```{r workflows}
lr_wf <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(train_rcp)
```

These are easy to explain:

1. I create a workflow for the model 
2. I add the model that I have initialised in the preceeding step
3. I add the recipe previously created

Next, I will kick off the training process:

```{r train_baseline}
lr_fit <- 
  lr_wf %>% 
  fit(data=train)

```

### Extracting the fitted data

I want to pull the data fits into a tibble I can explore. This can be done below:

```{r pull_fit}
lr_fitted <- lr_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

```

I will visualise this via a bar chart to observe my significant features: 
```{r viz_log_reg}

lr_fitted_add <- lr_fitted  %>% 
  mutate(Significance = ifelse(p.value < 0.05, 
                               "Significant", "Insignificant")) %>% 
  arrange(desc(p.value)) 
#Create a ggplot object to visualise significance
plot <- lr_fitted_add %>% 
  ggplot(mapping = aes(x=term, y=p.value, fill=Significance)) +
  geom_col() + theme(axis.text.x = element_text(
                                        face="bold", color="#0070BA", 
                                        size=8, angle=90)
                                                ) + labs(y="P value", x="Terms", 
                                                         title="P value significance chart",
                                                         subtitle="A chart to represent the significant variables in the model",
                                                         caption="Produced by Gary Hutson")

plotly::ggplotly(plot) 


```

## Training a tree based boosting model (XGBoost)

There are many ways to improve model performance, but the three main ways are:

1. Boosting
2. Bagging 
3. Stacking 

There are specific R packages for two of these - for bagging see baguette and for stacking see stacks.

### Set up model

Firstly, we are going to repeat the same process as above and then we are going to compare the results that we get from both models to make a decision about which one to push into production. 

This time I will hyperparameter tune the number of trees to grow and the depth of the leafs of the tree. For details of the maths underpinning this model, check out Josh Stamar's excellent videos: https://www.youtube.com/watch?v=ZVFeW798-2I.

```{r setup_xgboost}

xgboost_mod <- boost_tree(trees=tune(), tree_depth = tune()) %>% 
  set_mode('classification') %>% 
  set_engine('xgboost')

```

### Hyperparameter tuning and K-Fold Cross Validation

Here, as stated, I will do an iterative search for the best parameters to pass to my model:

```{r hp_search}
# Set the selected parameters in the grid
boost_grid <- dials::grid_regular(
  trees(), tree_depth(), levels=5 #Number of combinations to try
)
# Create the resampling method i.e. K Fold Cross Validation
folds <- vfold_cv(train, k=5)

```


## Create XGBoost workflow

I will now implement the workflow to manage the XGBoost model:

```{r xgboost_wf}
xgboost_wf <- workflow() %>%
  add_model(xgboost_mod) %>% 
  add_recipe(train_rcp)
```

Once I have this I can then go about iterating through the best combinations of fold and hyperparameter: 

```{r training_model, echo=FALSE, include=TRUE, results='hide', message=FALSE}
xgboost_fold <- xgboost_wf %>% 
  tune_grid(resamples = folds, grid=boost_grid)

head(collect_metrics(xgboost_fold))

```

We will now select the best model:

```{r grab_the_best_mod}
best_model <- xgboost_fold %>% 
  select_best('accuracy')
```

Visualising the results: 

```{r viz_hp}
xgboost_fold %>% 
  collect_metrics() %>% 
  mutate(tree_depth = factor(tree_depth)) %>% 
  ggplot(aes(trees, mean, color = tree_depth)) +
  geom_line(size=1.5, alpha=0.6) +
  geom_point(size=2) +
  facet_wrap(~ .metric, scales='free', nrow=2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option='plasma', begin=.9, end =0) + theme_minimal()
```
### Finalise the workflow and fit best model

I will now finalise my workflow by slecting the best hyperparameters for the job:

```{r get_best_model_and_hp}
final_wf <- 
  xgboost_wf %>% 
  finalize_workflow(best_model)

print(final_wf)

# Final fit of our fold and hyperparameter combination

final_xgboost_fit <- 
  final_wf %>% 
  last_fit(td_split)


```

### Collect metrics for evaluation 

The final step would be to collect the metrics for evaluation. We will dedicate a seperate section to the evaluation of our models:

```{r collected_metrics}
final_xgboost_fit %>% 
  collect_metrics()

```

Next we will look at the workflow fit:

```{r workflow_fit}

# This extracts the workflow fit
workflow_xgboost_fit <- final_xgboost_fit %>% 
  extract_workflow()

# This extracts the parsnip model
xgboost_model_fit <- final_xgboost_fit %>% 
  extract_fit_parsnip()
```
## Model evaluation

As we are following on from the xgboost model building, we will evaluate this first and then compare to our baseline model:

### Use fitted XGBoost model to predict on testing set

The aim here is to check that our `predictions` match up with our `ground truth` labels. The class labels will be determined by probabilities that are higher than 0.5, however we are going to tweak this threshold to only allow a label if the probability is greater than 0.7:

```{r eval_xgboost}
# Pass our test data through model
testing_fit_class <- predict(workflow_xgboost_fit, test)
testing_fit_probs <- predict(workflow_xgboost_fit, test, type='prob')
# Bind this on to our test data with the label to compare ground truth vs predicted
predictions<- cbind(test,testing_fit_probs, testing_fit_class) %>%
  dplyr::mutate(xgboost_model_pred=.pred_class,
                xgboost_model_prob=.pred_sick) %>% 
  dplyr::select(everything(), -c(.pred_class, .pred_negative)) %>% 
  dplyr::mutate(xgboost_class_custom = ifelse(xgboost_model_prob >0.7,"sick","negative")) %>% 
  dplyr::select(-.pred_sick)

```


### Use fitted logistic regression model to predict on test set

We are going to now append our predictions from our model we created as a baseline to append to the predictions we already have in the `predictions` data frame:

```{r lr_model_fit}
testing_lr_fit_probs <- predict(lr_fit, test, type='prob')
testing_lr_fit_class <- predict(lr_fit, test)

predictions<- cbind(predictions, testing_lr_fit_probs, testing_lr_fit_class)

predictions <- predictions %>% 
  dplyr::mutate(log_reg_model_pred=.pred_class,
                log_reg_model_prob=.pred_sick) %>% 
  dplyr::select(everything(), -c(.pred_class, .pred_negative)) %>% 
  dplyr::mutate(log_reg_class_custom = ifelse(log_reg_model_prob >0.7,"sick","negative")) %>% 
  dplyr::select(-.pred_sick)


# Get a head view of the finalised data
head(predictions)

```

### Evaluating the models with the ConfusionTableR package

The default `caret` confusion_matrix function saves everything as a string and doesn't allow you to work with the values from the output. 

This is the problem the ConfusionTableR package solves and means that you can easily store down the variables into a textual output, as and when needed. 


#### Evaluate Logistic Regression baseline

First, I will evaluate my baseline model using the package:

```{r baseline_mod_eval}
cm_lr <- ConfusionTableR::binary_class_cm(
  #Here you will have to cast to factor type as the tool expects factors
  train_labels = as.factor(predictions$log_reg_class_custom),
  truth_labels = as.factor(predictions$ThryroidClass),
  positive='sick', mode='everything'
  )

# View the confusion matrix native
cm_lr$confusion_matrix

```
The baseline model performs pretty well. You can see this is the result of fixing our imbalance. Let's work with it in a row wise fashion, as we can extract some metrics we my be interested in:

```{r extract_eval_data_lr}
# Get record level confusion matrix for logistic regression model
cm_rl_log_reg <- cm_lr$record_level_cm
accuracy_frame <- tibble(
  Accuracy=cm_rl_log_reg$Accuracy,
  Kappa=cm_rl_log_reg$Kappa,
  Precision=cm_rl_log_reg$Precision,
  Recall=cm_rl_log_reg$Recall
)

```

#### Evaluate XGBoost Model 

The next stage is to evaluate the XGBoost baseline. We will use this final evaluation to compare with our baseline model. `Note: in reality this would be compared across many models`:

```{r evaluate_xgboost_cm}
cm_xgb <- ConfusionTableR::binary_class_cm(
  #Here you will have to cast to factor type as the tool expects factors
  train_labels = as.factor(predictions$xgboost_class_custom),
  truth_labels = as.factor(predictions$ThryroidClass),
  positive='sick', mode='everything'
  )

# View the confusion matrix native
cm_xgb$confusion_matrix

```

I will now extract the predictions and then I will bind the predictions on to the original frame to view what the difference is:

```{r extract_eval_data}
# Get record level confusion matrix for the XGBoost model
cm_rl_xgboost <- cm_xgb$record_level_cm

accuracy_frame_xg <- tibble(
  Accuracy=cm_rl_xgboost$Accuracy,
  Kappa=cm_rl_xgboost$Kappa,
  Precision=cm_rl_xgboost$Precision,
  Recall=cm_rl_xgboost$Recall
)

# Bind the rows from the previous frame 
accuracy_frame <- rbind(accuracy_frame, accuracy_frame_xg)
rm(accuracy_frame_xg)

```

Comparing the two confusion matrices we have two different models, in reality we would test multiple models, with multiple hyperparameters and multiple splits. 

That is an example of how to rebalance and improve on the baseline model. Now I will take the fit from our test model and deploy with a new R MLOps package called `vetiver`.

## MLOps with Vetiver

The steps to deploy a model with vetiver is to:
1. Version 
2. Deploy
3. Monitor

The subsections hereunder will show you how to do this.

### Versioning with Vetiver

I will demonstrate how to deploy our original baseline model. At the moment vetiver serialisation of the model is not supported. The TidyModels team are addressing this and will update their GitHub ticket.

Initialising our vetiver model object:

```{r init_vetiver}
vet_lr_mod <- vetiver_model(lr_fit, "logistic_regression_model")

```

The next phase is to store and version our model, so if it is retrained, the version can be extracted to roll back to previous model serialisations:

```{r versioning}
library(pins)
model_board <- board_temp(versioned = TRUE)
model_board %>% vetiver::vetiver_pin_write(vet_lr_mod)
model_board %>% pin_versions("logistic_regression_model")
```

### Deploying with vetiver

We will create a restful API for our deployment of our logistic regression baseline model with `vetiver`.

#### Create a REST API

We will use Plumber here, as this allows for quickly deploying web services. See my tutorial on creating a REST API from scratch with Plumber: https://github.com/StatsGary/NHS_R_Community_Intro_to_Docker.

```{r plumberise}
#https://vetiver.rstudio.com/get-started/deploy.html
library(plumber)
pr() %>% 
  vetiver_api(vet_lr_mod)

# Write the plumber file
vetiver_write_plumber(model_board, 'logistic_regression_model')
```
The next step would be to generate some deployment docs. This can be done easily with vetiver.

### Generate Docker file with Vetiver

To generate your docker files, you can use the below command to generate the doc for container deployment in a Docker miroservice:

```{r dockerfile}
vetiver_write_docker(vet_lr_mod)
```
