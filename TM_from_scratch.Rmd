---
title: "Building a TidyModel for Classification from scratch"
author: "Gary Hutson - Head of Machine Learning"
date: "15/12/2021"

output:
  html_document:
    theme: lumen
    highlight: tango
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, echo=TRUE, include=FALSE}
library(MLDataR)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(stacks)
library(skimr)
library(purrr)
library(ggthemes)
library(modeldata)

```

# Data preparation 

The following steps will show you the steps to prepare the data.

## Load in the dataaset and view statistics

The dataset for our example will be the Thyroid dataset contained in the `MLDataR` package.

```{r dataset_include}
td <- MLDataR::thyroid_disease
td <- td %>% 
  dplyr::filter(patient_age <400)
skim(td)
```

## Clean imports

We will remove our null values for now, but you could impute these with methods such as MICE or mean/mode/median imputation methods, see:.

```{r clean_data}
td_clean <- td[complete.cases(td),]
dim(td_clean)
```

## View class distribution

Next we will view the class distribution of our classification task:

```{r class_disp}
table_class <- table(td_clean$ThryroidClass)
class_imbalance_original <- unclass(prop.table(table_class))[1:2]
print(class_imbalance_original)
```

We will do some over sampling on the sick cases later on in this tutorial, however this level of imbalance will lead to skewed ML models in terms of predicting most patients not to have a thyroid issue. 

Smote is the algorithm we will use for dealing with imbalance:

![SMOTE]()


# Explaratory Data Analysis (EDA)
<!--https://www.kaggle.com/code/statsgary/thyroid-disease-eda-classification-and-ensembling/edit-->

We will create a function to look at the distribution of the data:

```{r distribution_function}
# Get continuous variables only
subset <- td_clean %>% 
  dplyr::select(ThryroidClass, patient_age, TSH_reading, T3_reading,
                T4_reading, thyrox_util_rate_T4U_reading,
                FTI_reading)


# Bring in external file for visualisations
source('functions/visualisations.R')

# Use plot function
plot <- histoplotter(subset, ThryroidClass, 
                     chart_x_axis_lbl = 'Thyroid Class', 
                     chart_y_axis_lbl = 'Measures',boxplot_color = 'navy', 
                     boxplot_fill = '#89CFF0', box_fill_transparency = 0.2) 

# Add extras to plot
plot + ggthemes::theme_solarized() + theme(legend.position = 'none') + 
  scale_color_manual(values=c('negative' = 'red', 'positive' = 'blue'))


```

As you can see - we have a number of outliers in our continuous variables. To deal with this we will apply a standardisation method to bring that variability on to a similar scale by mean centering, or another technique, to reduce the affects of the statistical outliers. Other treatment options could be to expunge these from your analysis via anomaly / outlier detection techniques. 

# Model preparation

The next set of steps will be used to get the data ready for the training of our models - we will have a baseline model and compare against a model known for tearing the tabular data challenges on Kaggle. 

## Dividing our data into train/val/test samples

Now we will divide our data into training, validation and test samples:

```{r divide_and_conquer}
td_clean <- td_clean %>% 
  dplyr::mutate(ThryroidClass = as.factor(ThryroidClass)) %>% 
  #dplyr::select(-ref_src) %>% 
  drop_na()

# Split the dataset 
td_split <- rsample::initial_split(td_clean, 
                                   strata = ThryroidClass, 
                                   prop=0.9,
                                   breaks = 4)

train <- rsample::training(td_split)
test <- rsample::testing(td_split)

```

Okay, we have our training and testing sample. This sample will be used to assess how accurate the model is on the held out testing set. This will link to the evaluate metrics for the model. We will delve into that later on in this training.

## Making our first recipe

Recipes are a way to simplify the feature engineering process. Back in the old days you had to do each of these steps to your training data prior to fitting a model, especially using package such as `caret`. Now, you can speed this process up massively with the help of the recipes package. Let's whip up our first recipe:

```{r training_recipe}

train_rcp <- recipes::recipe(ThryroidClass ~ ., data=train) %>% 
  step_dummy(ref_src, role='predictor') %>% 
  # Upsample data
  themis::step_smote(ThryroidClass, over_ratio = 0.97, neighbors = 3) %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors())  
 # Remove predictors that have little effect on target


# Prep and bake the recipe so we can view this as a seperate data frame
training_df <- train_rcp %>% 
  prep() %>% 
  juice()

# Class imbalance resolved
class_imbalance_after_smote <- unclass(prop.table(table(training_df$ThryroidClass)))[1:2]


```

As we applied Synthetic Minority Oversampling - which is a nearest neighbours method of oversampling we need to check what has happened to our binary labels (negative or sick):

```{r class_imbalance}
imbalance_frame <- tibble(class_imbalance_original,
           class_imbalance_after_smote)

print(imbalance_frame)

```
This technique is not always successful, due to the severity of the imbalance, the representation of the sick class might make the overall distribution imbalanced. There are many methods to deal with imbalance - here are a few papers:

- 
- 

