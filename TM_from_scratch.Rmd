---
title: "Building a TidyModel for Classification from scratch"
author: "Gary Hutson - Head of Machine Learning"

output:
  html_document:
    theme: paper
    highlight: textmate
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, echo=TRUE, include=FALSE}
library(MLDataR)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(stacks)
library(skimr)
library(purrr)
library(ggthemes)
library(modeldata)

```

# Data preparation 

The following steps will show you the steps to prepare the data.

## Load in the dataaset and view statistics

The dataset for the example will be the Thyroid dataset contained in the `MLDataR` package.

```{r dataset_include}
td <- MLDataR::thyroid_disease
td <- td %>% 
  dplyr::filter(patient_age <400)
skim(td)
```

## Clean imports

We will remove the null values for now, but you could impute these with methods such as MICE or mean/mode/median imputation methods, see:.

```{r clean_data}
td_clean <- td[complete.cases(td),]
dim(td_clean)
```

## View class distribution

Next we will view the class distribution of the classification task:

```{r class_disp}
table_class <- table(td_clean$ThryroidClass)
class_imbalance_original <- unclass(prop.table(table_class))[1:2]
print(class_imbalance_original)
```

We will do some over sampling on the sick cases later on in this tutorial, however this level of imbalance will lead to skewed ML models in terms of predicting most patients not to have a thyroid issue. 

Smote is the algorithm we will use for dealing with imbalance:

![](man/fig/test_smote.png)


# Explaratory Data Analysis (EDA)
<!--https://www.kaggle.com/code/statsgary/thyroid-disease-eda-classification-and-ensembling/edit-->

We will create a function to look at the distribution of the data:

```{r distribution_function}
# Get continuous variables only
subset <- td_clean %>% 
  dplyr::select(ThryroidClass, patient_age, TSH_reading, T3_reading,
                T4_reading, thyrox_util_rate_T4U_reading,
                FTI_reading)


# Bring in external file for visualisations
source('functions/visualisations.R')

# Use plot function
plot <- histoplotter(subset, ThryroidClass, 
                     chart_x_axis_lbl = 'Thyroid Class', 
                     chart_y_axis_lbl = 'Measures',boxplot_color = 'navy', 
                     boxplot_fill = '#89CFF0', box_fill_transparency = 0.2) 

# Add extras to plot
plot + ggthemes::theme_solarized() + theme(legend.position = 'none') + 
  scale_color_manual(values=c('negative' = 'red', 'positive' = 'blue'))


```

As you can see - we have a number of outliers in the continuous variables. To deal with this we will apply a standardisation method to bring that variability on to a similar scale by mean centering, or another technique, to reduce the affects of the statistical outliers. Other treatment options could be to expunge these from the analysis via anomaly / outlier detection techniques. 

# Model preparation

The next set of steps will be used to get the data ready for the training of the models - we will have a baseline model and compare against a model known for tearing the tabular data challenges on Kaggle. 

## Dividing the data into train/val/test samples

Now we will divide the data into training, validation and test samples:

```{r divide_and_conquer}
td_clean <- td_clean %>% 
  dplyr::mutate(ThryroidClass = as.factor(ThryroidClass)) %>% 
  #dplyr::select(-ref_src) %>% 
  drop_na()

# Split the dataset 
td_split <-initial_split(td_clean, 
                                   strata = ThryroidClass, 
                                   prop=0.9,
                                   breaks = 4)

train <- training(td_split)
test <- testing(td_split)

```

Okay, we have the training and testing sample. This sample will be used to assess how accurate the model is on the held out testing set. This will link to the evaluate metrics for the model. We will delve into that later on in this training.

## Making the first recipe

Recipes are a way to simplify the feature engineering process. Back in the old days you had to do each of these steps to the training data prior to fitting a model, especially using package such as `caret`. Now, you can speed this process up massively with the help of the recipes package. Let's whip up the first recipe:

```{r training_recipe}

train_rcp <- recipes::recipe(ThryroidClass ~ ., data=train) %>% 
  step_dummy(ref_src, role='predictor') %>% 
  # Upsample data
  themis::step_smote(ThryroidClass, over_ratio = 0.97, neighbors = 3) %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors())  
 # Remove predictors that have little effect on target


# Prep and bake the recipe so we can view this as a seperate data frame
training_df <- train_rcp %>% 
  prep() %>% 
  juice()

# Class imbalance resolved
class_imbalance_after_smote <- unclass(prop.table(table(training_df$ThryroidClass)))[1:2]


```

As we applied Synthetic Minority Oversampling - which is a nearest neighbours method of oversampling we need to check what has happened to the binary labels (negative or sick):

```{r class_imbalance}
imbalance_frame <- tibble(class_imbalance_original,
           class_imbalance_after_smote)

print(imbalance_frame)

```
This technique is not always successful, due to the severity of the imbalance, the representation of the sick class might make the overall distribution imbalanced. 

# Model training 

In this example I will create a baseline model and compare against one further classifier, for the sake of brevity. However, in ML challenges it is common to try many different classifiers and pit them against each other in the evaluation stages. 


## Training the Logistic Regression baseline model

The theory is that if a simple linear classifier does a better job than a more complex algorithm, then stick with good old logistic regression. I won't cover the mathematics of logistic regression, but it follows very closely to a linear regression equation, with the addition that there is a log link function used to turn it from a regressor into a classifier.

### Initialising the model

Here I use [Parsnip](https://parsnip.tidymodels.org/) to search for the list of [available models](https://www.tidymodels.org/find/parsnip/):

```{r logistic_reg}
lr_mod <- parsnip::logistic_reg() %>% 
  set_engine('glm')

print(lr_mod)

```

### Creating the model workflow

We will use workflows to create the model workflow:

```{r workflows}
lr_wf <-
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(train_rcp)
```

These are easy to explain:

1. I create a workflow for the model 
2. I add the model that I have initialised in the preceeding step
3. I add the recipe previously created

Next, I will kick off the training process:

```{r train_baseline}
lr_fit <- 
  lr_wf %>% 
  fit(data=train)

```

### Extracting the fitted data

I want to pull the data fits into a tibble I can explore. This can be done below:

```{r pull_fit}
lr_fitted <- lr_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

```

I will visualise this via a bar chart to observe my significant features: 
```{r viz_log_reg}

lr_fitted_add <- lr_fitted  %>% 
  mutate(Significance = ifelse(p.value < 0.05, 
                               "Significant", "Insignificant")) %>% 
  arrange(desc(p.value)) 
#Create a ggplot object to visualise significance
plot <- lr_fitted_add %>% 
  ggplot(mapping = aes(x=term, y=p.value, fill=Significance)) +
  geom_col() + theme(axis.text.x = element_text(
                                        face="bold", color="#0070BA", 
                                        size=8, angle=90)
                                                ) + labs(y="P value", x="Terms", 
                                                         title="P value significance chart",
                                                         subtitle="A chart to represent the significant variables in the model",
                                                         caption="Produced by Gary Hutson")

plotly::ggplotly(plot) 


```

## Training a tree based boosting model (XGBoost)

There are many ways to improve model performance, but the three main ways are:

1. Boosting
2. Bagging 
3. Stacking 

There are specific R packages for two of these - for bagging see baguette and for stacking see stacks.

### Set up model

Firstly, we are going to repeat the same process as above and then we are going to compare the results that we get from both models to make a decision about which one to push into production. 

This time I will hyperparameter tune the number of trees to grow and the depth of the leafs of the tree. For details of the maths underpinning this model, check out Josh Stamar's excellent videos: https://www.youtube.com/watch?v=ZVFeW798-2I.

```{r setup_xgboost}

xgboost_mod <- boost_tree(trees=tune(), tree_depth = tune()) %>% 
  set_mode('classification') %>% 
  set_engine('xgboost')

```

### Hyperparameter tuning and K-Fold Cross Validation

Here, as stated, I will do an iterative search for the best parameters to pass to my model:

```{r hp_search}
# Set the selected parameters in the grid
boost_grid <- dials::grid_regular(
  trees(), tree_depth(), levels=5 #Number of combinations to try
)
# Create the resampling method i.e. K Fold Cross Validation
folds <- vfold_cv(train, k=5)

```


## Create XGBoost workflow

I will now implement the workflow to manage the XGBoost model:

```{r xgboost_wf}
xgboost_wf <- workflow() %>%
  add_model(xgboost_mod) %>% 
  add_recipe(train_rcp)
```

Once I have this I can then go about iterating through the best combinations of fold and hyperparameter: 

```{r training_model, echo=FALSE}
xgboost_fold <- xgboost_wf %>% 
  tune_grid(resamples = folds, grid=boost_grid)

head(collect_metrics(xgboost_fold))

```
```{r grab_the_best_mod}
best_model <- xgboost_fold %>% 
  select_best('accuracy')
```

Visualising the results: 

```{r viz_hp}
xgboost_fold %>% 
  collect_metrics() %>% 
  mutate(tree_depth = factor(tree_depth)) %>% 
  ggplot(aes(trees, mean, color = tree_depth)) +
  geom_line(size=1.5, alpha=0.6) +
  geom_point(size=2) +
  facet_wrap(~ .metric, scales='free', nrow=2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option='plasma', begin=.9, end =0) + theme_minimal()
```
### Finalise the workflow and fit best model

I will now finalise my workflow by slecting the best hyperparameters for the job:

```{r get_best_model_and_hp}
final_wf <- 
  xgboost_wf %>% 
  finalize_workflow(best_model)

print(final_wf)

# Final fit of our fold and hyperparameter combination

final_xgboost_fit <- 
  final_wf %>% 
  last_fit(td_split)

```

### Collect metrics for evaluation 

The final step would be to collect the metrics for evaluation. We will dedicate a seperate section to the evaluation of our models:

```{r collected_metrics}
final_xgboost_fit %>% 
  collect_metrics()

```

Next we will look at the workflow fit:

```{r workflow_fit}
workflow_xgboost_fit <- final_xgboost_fit %>% 
  extract_workflow()
```
## Model evaluation

As we are following on from the xgboost model building, we will evaluate this first and then compare to our baseline model:

### Use fitted model to predict on testing set

The aim here is to check that our `predictions` match up with our `ground truth` labels. 

```{r eval_xgboost}
# Pass our test data through model
testing_fit_class <- predict(workflow_xgboost_fit, test)
testing_fit_probs <- predict(workflow_xgboost_fit, test, type='prob')
# Bind this on to our test data with the label to compare ground truth vs predicted
predictions<- cbind(test, testing_fit, testing_fit_probs) %>% 
  dplyr::mutate(xgboost_model_pred=.pred_class) %>% 
  dplyr::select(everything(), -.pred_class)

```








